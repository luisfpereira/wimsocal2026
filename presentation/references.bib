@book{amari2016,
  title     = {Information {Geometry} and {Its} {Applications}},
  isbn      = {978-4-431-55978-8},
  abstract  = {This is the first comprehensive book on information geometry, written by the founder of the field. It begins with an elementary introduction to dualistic geometry and proceeds to a wide range of applications, covering information science, engineering, and neuroscience. It consists of four parts, which on the whole can be read independently. A manifold with a divergence function is first introduced, leading directly to dualistic structure, the heart of information geometry. This part (Part I) can be apprehended without any knowledge of differential geometry. An intuitive explanation of modern differential geometry then follows in Part II, although the book is for the most part understandable without modern differential geometry. Information geometry of statistical inference, including time series analysis and semiparametric estimation (the Neyman–Scott problem), is demonstrated concisely in Part III. Applications addressed in Part IV include hot current topics in machine learning, signal processing, optimization, and neural networks. The book is interdisciplinary, connecting mathematics, information sciences, physics, and neurosciences, inviting readers to a new world of information and geometry. This book is highly recommended to graduate students and researchers who seek new mathematical methods and tools useful in their own fields.},
  language  = {en},
  publisher = {Springer},
  author    = {Amari, Shun-ichi},
  month     = feb,
  year      = {2016},
  note      = {Google-Books-ID: UkSFCwAAQBAJ},
  keywords  = {Mathematics / Applied, Mathematics / Probability \& Statistics / General, Mathematics / Geometry / General, Mathematics / Geometry / Differential},
  file      = {amari2016.pdf:/home/luisfpereira/Zotero/storage/JVDPCT6I/amari2016.pdf:application/pdf}
}

@article{dietrich2020,
  title    = {On the {Koopman} {Operator} of {Algorithms}},
  volume   = {19},
  issn     = {1536-0040},
  url      = {https://epubs.siam.org/doi/10.1137/19M1277059},
  doi      = {10.1137/19M1277059},
  abstract = {A systematic mathematical framework for the study of numerical algorithms would allow comparisons, facilitate conjugacy arguments, and enable the discovery of improved, accelerated, data-driven algorithms. Over the course of the past century, the Koopman operator has provided a mathematical framework for the study of dynamical systems which facilitates conjugacy arguments and can provide eﬃcient reduced descriptions. More recently, numerical approximations of the operator have enabled the analysis of a large number of deterministic and stochastic dynamical systems in a completely data-driven, essentially equation-free pipeline. Discrete- or continuous-time numerical algorithms (integrators, nonlinear equation solvers, optimization algorithms are themselves dynamical systems. In this paper, we use this insight to leverage the Koopman operator framework in the data-driven study of such algorithms and discuss beneﬁts for analysis and acceleration of numerical computation. For algorithms acting on high-dimensional spaces by quickly contracting them toward low-dimensional manifolds, we demonstrate how basis functions adapted to the data help to construct eﬃcient reduced representations of the operator. Our illustrative examples include the gradient descent and Nesterov optimization algorithms as well as the Newton–Raphson algorithm.},
  language = {en},
  number   = {2},
  urldate  = {2026-02-13},
  journal  = {SIAM Journal on Applied Dynamical Systems},
  author   = {Dietrich, Felix and Thiem, Thomas N. and Kevrekidis, Ioannis G.},
  month    = jan,
  year     = {2020},
  pages    = {860--885},
  file     = {PDF:/home/luisfpereira/Zotero/storage/7H2SRGSU/Dietrich et al. - 2020 - On the Koopman Operator of Algorithms.pdf:application/pdf}
}


@article{quinn2023,
  title      = {Information geometry for multiparameter models: new perspectives on the origin of simplicity},
  volume     = {86},
  issn       = {0034-4885, 1361-6633},
  shorttitle = {Information geometry for multiparameter models},
  url        = {https://iopscience.iop.org/article/10.1088/1361-6633/aca6f8},
  doi        = {10.1088/1361-6633/aca6f8},
  abstract   = {Abstract
                
                Complex models in physics, biology, economics, and engineering are often
                sloppy
                , meaning that the model parameters are not well determined by the model predictions for collective behavior. Many parameter combinations can vary over decades without significant changes in the predictions. This review uses information geometry to explore sloppiness and its deep relation to emergent theories. We introduce the
                model manifold
                of predictions, whose coordinates are the model parameters. Its
                hyperribbon
                structure explains why only a few parameter combinations matter for the behavior. We review recent rigorous results that connect the hierarchy of hyperribbon widths to approximation theory, and to the smoothness of model predictions under changes of the control variables. We discuss recent geodesic methods to find simpler models on nearby boundaries of the model manifold—emergent theories with fewer parameters that explain the behavior equally well. We discuss a Bayesian prior which optimizes the mutual information between model parameters and experimental data, naturally favoring points on the emergent boundary theories and thus simpler models. We introduce a ‘projected maximum likelihood’ prior that efficiently approximates this optimal prior, and contrast both to the poor behavior of the traditional Jeffreys prior. We discuss the way the renormalization group coarse-graining in statistical mechanics introduces a flow of the model manifold, and connect stiff and sloppy directions along the model manifold with relevant and irrelevant eigendirections of the renormalization group. Finally, we discuss recently developed ‘intensive’ embedding methods, allowing one to visualize the predictions of arbitrary probabilistic models as low-dimensional projections of an isometric embedding, and illustrate our method by generating the model manifold of the Ising model.},
  number     = {3},
  urldate    = {2026-02-17},
  journal    = {Reports on Progress in Physics},
  author     = {Quinn, Katherine N and Abbott, Michael C and Transtrum, Mark K and Machta, Benjamin B and Sethna, James P},
  month      = mar,
  year       = {2023},
  keywords   = {Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Condensed Matter - Soft Condensed Matter, Condensed Matter - Statistical Mechanics, wanttoread},
  pages      = {035901},
  file       = {Full text:/home/luisfpereira/Zotero/storage/DA9NEYLG/quinn2023.pdf:application/pdf;Preprint PDF:/home/luisfpereira/Zotero/storage/J5PYDSDI/Quinn et al. - 2022 - Information geometry for multiparameter models New perspectives on the origin of simplicity.pdf:application/pdf;Snapshot:/home/luisfpereira/Zotero/storage/Z44M8ST3/2111.html:text/html}
}


@article{transtrum2011,
  title    = {Geometry of nonlinear least squares with applications to sloppy models and optimization},
  volume   = {83},
  url      = {https://link.aps.org/doi/10.1103/PhysRevE.83.036701},
  doi      = {10.1103/PhysRevE.83.036701},
  abstract = {Parameter estimation by nonlinear least-squares minimization is a common problem that has an elegant geometric interpretation: the possible parameter values of a model induce a manifold within the space of data predictions. The minimization problem is then to find the point on the manifold closest to the experimental data. We show that the model manifolds of a large class of models, known as sloppy models, have many universal features; they are characterized by a geometric series of widths, extrinsic curvatures, and parameter-effect curvatures, which we describe as a hyper-ribbon. A number of common difficulties in optimizing least-squares problems are due to this common geometric structure. First, algorithms tend to run into the boundaries of the model manifold, causing parameters to diverge or become unphysical before they have been optimized. We introduce the model graph as an extension of the model manifold to remedy this problem. We argue that appropriate priors can remove the boundaries and further improve the convergence rates. We show that typical fits will have many evaporated parameters unless the data are very accurately known. Second, “bare” model parameters are usually ill-suited to describing model behavior; cost contours in parameter space tend to form hierarchies of plateaus and long narrow canyons. Geometrically, we understand this inconvenient parametrization as an extremely skewed coordinate basis and show that it induces a large parameter-effect curvature on the manifold. By constructing alternative coordinates based on geodesic motion, we show that these long narrow canyons are transformed in many cases into a single quadratic, isotropic basin. We interpret the modified Gauss-Newton and Levenberg-Marquardt fitting algorithms as an Euler approximation to geodesic motion in these natural coordinates on the model manifold and the model graph, respectively. By adding a geodesic acceleration adjustment to these algorithms, we alleviate the difficulties from parameter-effect curvature, improving both efficiency and success rates at finding good fits.},
  number   = {3},
  urldate  = {2025-08-27},
  journal  = {Physical Review E},
  author   = {Transtrum, Mark K. and Machta, Benjamin B. and Sethna, James P.},
  month    = mar,
  year     = {2011},
  note     = {Publisher: American Physical Society},
  pages    = {036701},
  file     = {Full Text PDF:/home/luisfpereira/Zotero/storage/STY3V5FT/Transtrum et al. - 2011 - Geometry of nonlinear least squares with applications to sloppy models and optimization.pdf:application/pdf}
}

@book{dryden2016,
  title      = {Statistical {Shape} {Analysis}: {With} {Applications} in {R}},
  isbn       = {978-0-470-69962-1},
  shorttitle = {Statistical {Shape} {Analysis}},
  abstract   = {A thoroughly revised and updated edition of this introduction to modern statistical methods for shape analysis Shape analysis is an important tool in the many disciplines where objects are compared using geometrical features. Examples include comparing brain shape in schizophrenia; investigating protein molecules in bioinformatics; and describing growth of organisms in biology. This book is a significant update of the highly-regarded Statistical Shape Analysis by the same authors. The new edition lays the foundations of landmark shape analysis, including geometrical concepts and statistical techniques, and extends to include analysis of curves, surfaces, images and other types of object data. Key definitions and concepts are discussed throughout, and the relative merits of different approaches are presented. The authors have included substantial new material on recent statistical developments and offer numerous examples throughout the text. Concepts are introduced in an accessible manner, while retaining sufficient detail for more specialist statisticians to appreciate the challenges and opportunities of this new field. Computer code has been included for instructional use, along with exercises to enable readers to implement the applications themselves in R and to follow the key ideas by hands-on analysis.  Offers a detailed yet accessible treatment of statistical methods for shape analysis Includes numerous examples and applications from many disciplines Provides R code for implementing the examples Covers a wide variety of recent developments in shape analysis  Shape Analysis, with Applications in R will offer a valuable introduction to this fast-moving research area for statisticians and other applied scientists working in diverse areas, including archaeology, bioinformatics, biology, chemistry, computer science, medicine, morphometics and image analysis.},
  language   = {en},
  publisher  = {John Wiley \& Sons},
  author     = {Dryden, Ian L. and Mardia, Kanti V.},
  month      = sep,
  year       = {2016},
  note       = {Google-Books-ID: jGstCwAAQBAJ},
  keywords   = {Mathematics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Probability \& Statistics / General},
  file       = {dryden2016.pdf:/home/luisfpereira/Zotero/storage/588KAMES/dryden2016.pdf:application/pdf}
}

@inproceedings{glaunes2004,
  title      = {Diffeomorphic matching of distributions: a new approach for unlabelled point-sets and sub-manifolds matching},
  volume     = {2},
  shorttitle = {Diffeomorphic matching of distributions},
  url        = {https://ieeexplore.ieee.org/document/1315234},
  doi        = {10.1109/CVPR.2004.1315234},
  booktitle  = {Proceedings of the 2004 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2004. {CVPR} 2004.},
  author     = {Glaunes, J. and Trouve, A. and Younes, L.},
  month      = jun,
  year       = {2004},
  note       = {ISSN: 1063-6919},
  keywords   = {Biomedical imaging, Clustering algorithms, Computational Intelligence Society, Computer vision, Interpolation, Noise robustness, Optimal matching, Particle measurements, Probability distribution, Spline},
  pages      = {II--II}
}

@inproceedings{kaltenmark2017,
  title  = {A {General} {Framework} for {Curve} and {Surface} {Comparison} and {Registration} {With} {Oriented} {Varifolds}},
  url    = {https://openaccess.thecvf.com/content_cvpr_2017/html/Kaltenmark_A_General_Framework_CVPR_2017_paper.html},
  author = {Kaltenmark, Irene and Charlier, Benjamin and Charon, Nicolas},
  year   = {2017},
  pages  = {3346--3355}
}



@inproceedings{vaillant2005,
  address   = {Berlin, Heidelberg},
  title     = {Surface {Matching} via {Currents}},
  isbn      = {978-3-540-31676-3},
  doi       = {10.1007/11505730_32},
  language  = {en},
  booktitle = {Information {Processing} in {Medical} {Imaging}},
  publisher = {Springer},
  author    = {Vaillant, Marc and Glaunès, Joan},
  editor    = {Christensen, Gary E. and Sonka, Milan},
  year      = {2005},
  keywords  = {Distance Graph, Geometric Measure Theory, Medical Image Data, Surface Match, Triangular Mesh},
  pages     = {381--392}
}


@article{charon2013,
  title   = {The {Varifold} {Representation} of {Nonoriented} {Shapes} for {Diffeomorphic} {Registration}},
  volume  = {6},
  url     = {https://epubs.siam.org/doi/abs/10.1137/130918885},
  doi     = {10.1137/130918885},
  number  = {4},
  journal = {SIAM Journal on Imaging Sciences},
  author  = {Charon, Nicolas and Trouvé, Alain},
  month   = jan,
  year    = {2013},
  note    = {Publisher: Society for Industrial and Applied Mathematics},
  pages   = {2547--2580}
}



@article{charlier2017,
  title    = {The {Fshape} {Framework} for the {Variability} {Analysis} of {Functional} {Shapes}},
  volume   = {17},
  issn     = {1615-3383},
  url      = {https://doi.org/10.1007/s10208-015-9288-2},
  doi      = {10.1007/s10208-015-9288-2},
  language = {en},
  number   = {2},
  journal  = {Foundations of Computational Mathematics},
  author   = {Charlier, B. and Charon, N. and Trouvé, A.},
  month    = apr,
  year     = {2017},
  keywords = {Shape analysis, 58E50, 68U05, 49M25, 49Q20, 58B32, 68U10, Atlas estimation algorithms, Large deformation models, Metamorphoses, Signals on manifolds, Varifolds},
  pages    = {287--357}
}


@incollection{charon2020,
  title      = {Fidelity metrics between curves and surfaces: currents, varifolds, and normal cycles},
  isbn       = {978-0-12-814725-2},
  shorttitle = {12 - {Fidelity} metrics between curves and surfaces},
  url        = {https://www.sciencedirect.com/science/article/pii/B9780128147252000212},
  language   = {en},
  booktitle  = {Riemannian {Geometric} {Statistics} in {Medical} {Image} {Analysis}},
  publisher  = {Academic Press},
  author     = {Charon, Nicolas and Charlier, Benjamin and Glaunès, Joan and Gori, Pietro and Roussillon, Pierre},
  editor     = {Pennec, Xavier and Sommer, Stefan and Fletcher, Tom},
  month      = jan,
  year       = {2020},
  doi        = {10.1016/B978-0-12-814725-2.00021-2},
  keywords   = {computational anatomy, Current, discrete inner product, kernel metric, normal cycle, reproducing kernel Hilbert space, unit normal bundle, varifold},
  pages      = {441--477}
}



@article{hartman2023,
  title      = {Elastic {Shape} {Analysis} of {Surfaces} with {Second}-{Order} {Sobolev} {Metrics}: {A} {Comprehensive} {Numerical} {Framework}},
  volume     = {131},
  issn       = {1573-1405},
  shorttitle = {Elastic {Shape} {Analysis} of {Surfaces} with {Second}-{Order} {Sobolev} {Metrics}},
  url        = {https://doi.org/10.1007/s11263-022-01743-0},
  doi        = {10.1007/s11263-022-01743-0},
  language   = {en},
  number     = {5},
  journal    = {International Journal of Computer Vision},
  author     = {Hartman, Emmanuel and Sukurdeep, Yashil and Klassen, Eric and Charon, Nicolas and Bauer, Martin},
  month      = may,
  year       = {2023},
  keywords   = {Parallel transport, Elastic shape analysis, Invariant Sobolev metrics, Karcher mean, Partial matching, Varifold},
  pages      = {1183--1209}
}


@article{bauer2024curves,
  title      = {Elastic {Metrics} on {Spaces} of {Euclidean} {Curves}: {Theory} and {Algorithms}},
  volume     = {34},
  issn       = {1432-1467},
  shorttitle = {Elastic {Metrics} on {Spaces} of {Euclidean} {Curves}},
  url        = {https://doi.org/10.1007/s00332-024-10035-5},
  doi        = {10.1007/s00332-024-10035-5},
  language   = {en},
  number     = {3},
  journal    = {Journal of Nonlinear Science},
  author     = {Bauer, Martin and Charon, Nicolas and Klassen, Eric and Kurtek, Sebastian and Needham, Tom and Pierron, Thomas},
  month      = apr,
  year       = {2024},
  keywords   = {68T10, Shape analysis, 53Z50, Elastic metrics, 49Q10, Infinite-dimensional Riemannian geometry, Metric learning},
  pages      = {56}
}



@article{guigui2022geomstats,
  title      = {Introduction to {Riemannian} {Geometry} and {Geometric} {Statistics}: from basic theory to implementation with {Geomstats}},
  shorttitle = {Introduction to {Riemannian} {Geometry} and {Geometric} {Statistics}},
  url        = {https://hal.inria.fr/hal-03766900},
  language   = {en},
  journal    = {Foundations and Trends in Machine Learning},
  author     = {Guigui, Nicolas and Miolane, Nina and Pennec, Xavier},
  year       = {2022}
}



@misc{brigant2022infogeom,
  title     = {Parametric information geometry with the package {Geomstats}},
  url       = {http://arxiv.org/abs/2211.11643},
  doi       = {10.48550/arXiv.2211.11643},
  publisher = {arXiv},
  author    = {Brigant, Alice Le and Deschamps, Jules and Collas, Antoine and Miolane, Nina},
  month     = nov,
  year      = {2022},
  note      = {arXiv:2211.11643 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Mathematical Software}
}


@phdthesis{tumpach2022,
  type       = {thesis},
  title      = {Some aspects of infinite-dimensional {Geometry}: {Theory} and {Applications}},
  shorttitle = {Some aspects of infinite-dimensional {Geometry}},
  url        = {https://hal.science/tel-04052429},
  language   = {en},
  school     = {Lille University},
  author     = {Tumpach, Alice Barbara},
  month      = dec,
  year       = {2022}
}



@unpublished{crane2018,
  title  = {Discrete differential geometry: {An} applied introduction},
  url    = {https://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf},
  author = {Crane, Keenan},
  year   = {2018}
}


@misc{pereira2024shapes,
  title     = {Learning from landmarks, curves, surfaces, and shapes in {Geomstats}},
  url       = {http://arxiv.org/abs/2406.10437},
  doi       = {10.48550/arXiv.2406.10437},
  abstract  = {We introduce the shape module of the Python package Geomstats to analyze shapes of objects represented as landmarks, curves and surfaces across fields of natural sciences and engineering. The shape module first implements widely used shape spaces, such as the Kendall shape space, as well as elastic spaces of discrete curves and surfaces. The shape module further implements the abstract mathematical structures of group actions, fiber bundles, quotient spaces and associated Riemannian metrics which allow users to build their own shape spaces. The Riemannian geometry tools enable users to compare, average, interpolate between shapes inside a given shape space. These essential operations can then be leveraged to perform statistics and machine learning on shape data. We present the object-oriented implementation of the shape module along with illustrative examples and show how it can be used to perform statistics and machine learning on shape spaces.},
  urldate   = {2025-02-10},
  publisher = {arXiv},
  author    = {Pereira, Luís F. and Brigant, Alice Le and Myers, Adele and Hartman, Emmanuel and Khan, Amil and Tuerkoen, Malik and Dold, Trey and Gu, Mengyang and Suárez-Serrato, Pablo and Miolane, Nina},
  month     = jun,
  year      = {2024},
  note      = {arXiv:2406.10437 [cs]},
  keywords  = {Computer Science - Graphics, Computer Science - Mathematical Software, Mathematics - Differential Geometry}
}



@phdthesis{thanwerdas2022,
  type     = {Theses},
  title    = {Riemannian and stratified geometries of covariance and correlation matrices},
  url      = {https://hal.archives-ouvertes.fr/tel-03698752},
  urldate  = {2022-09-29},
  school   = {Université Côte d'Azur},
  author   = {Thanwerdas, Yann},
  month    = may,
  year     = {2022},
  keywords = {Riemannian geometry, Geodesics, Covariance matrix, Correlation matrix, Stratified spaces}
}
